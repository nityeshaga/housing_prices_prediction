{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5fb63261-70c8-41f4-b0cc-c7be2a0ead5b",
    "_uuid": "5fd8ee3dd2d5c72f2f9969adfb5229bb31e6b204"
   },
   "source": [
    "[The detailed analysis](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf) provided by the dataset author has helped in creating this kernel.  \n",
    "Also [here's a link](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt) to the detailed description of the dataset.\n",
    "\n",
    "These kernels were of great help to me:\n",
    "\n",
    "* https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\n",
    "* https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard \n",
    "* https://www.kaggle.com/apapiu/regularized-linear-models\n",
    "* https://www.kaggle.com/humananalog/xgboost-lasso\n",
    "* https://www.kaggle.com/poonaml/house-prices-data-exploration-and-visualisation\n",
    "\n",
    "I am new to data science. It would be really awesome if you could tell me the mistakes and how I could improve the kernel in the comments below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "234fd40cf10a139dbb26deaa833f0a8155cd2bd7"
   },
   "source": [
    "Here's the approach that I take in this kernel:\n",
    "\n",
    "**1. Data Exploration -**\n",
    "    Here, I separate out the features as belonging to one of the 3 classes -\n",
    "    \n",
    "    * `num_cont_columns` for numerical continuous features\n",
    "    * `num_discrete_columns` for numerical discrete features\n",
    "    * `categ_columns` for categorical features\n",
    "       \n",
    "   I do this because there are different steps of preprocessing required for each of those features. By separating them, I am able to create a clean analysis of the data.\n",
    "\n",
    "**2. Preprocessing -**\n",
    "    As I said, I divide the preprocessing stage in 3 steps where I analyse-\n",
    "    \n",
    "     * Continuous valued numeric features\n",
    "     * Discrete valued numeric features\n",
    "     * And finally, the Categorical features\n",
    "        \n",
    "   In each of those steps, I do the missing values imputation and feature engineering as required.\n",
    "\n",
    "**3. Modelling -**\n",
    "\n",
    "   I will train the following regression models -\n",
    "    * Simple regression\n",
    "    * Ridge regression\n",
    "    * ElasticNet regression\n",
    "    * Lasso regression\n",
    "    \n",
    "   And the following ensemble models - \n",
    "    * Random Forest\n",
    "    * XGBoost\n",
    "    \n",
    "    Finally, I took the top 3 best performing models and stacked them up for the final submission model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../input/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-954a21a8d2ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../input/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "train_id = df_train['Id']\n",
    "test_id = df_test['Id']\n",
    "\n",
    "df_train.drop(columns=['Id'], inplace=True)\n",
    "df_test.drop(columns=['Id'], inplace=True)\n",
    "\n",
    "ntrain = df_train.shape[0]\n",
    "ntest = df_test.shape[0]\n",
    "\n",
    "all_data = pd.concat((df_train, df_test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc4b6ce6-e561-45fe-a51f-a0bd789539a8",
    "_kg_hide-input": true,
    "_uuid": "cacd492b4a92e680360a3c5a34bc5bb1b9b82fde"
   },
   "outputs": [],
   "source": [
    "print(df_test.shape, df_train.shape, all_data.shape)\n",
    "print(df_test.shape[0] + df_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59580d12-7960-4d57-804c-9803fdb4bf92",
    "_uuid": "0076baa551fd9f05227e48073ac328e7442da4b3"
   },
   "source": [
    "# 1. Data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "805d2c45bc60863e07449d64057f0a3e37b3070d"
   },
   "outputs": [],
   "source": [
    "# see the decoration\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "218ac04f-6849-4f9b-bda1-ec2410a304bd",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "89a0ac37672cdee15a878091ac9c32040b752a6d"
   },
   "outputs": [],
   "source": [
    "# do this in order to view all the columns, otherwise pandas just shows a summary\n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "# Lets see what the data looks like\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "8e1f29594871cb217d4a3f397b6e24797df4b2f0"
   },
   "outputs": [],
   "source": [
    "df_train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "100c8cfb-1b24-43d0-b79b-10b312555364",
    "_uuid": "359675bd0f2a66559f16c7154334409dccbb8f03"
   },
   "source": [
    "There are a total of 80 unique features in the dataset.\n",
    "\n",
    "In the detailed description that the dataset, the author divided the features into 4 types:\n",
    "\n",
    "* Numerical:\n",
    "   3. Discrete (14)\n",
    "   4. Continuous (20)\n",
    "* Categorical:\n",
    "   1. Nominal (23)\n",
    "   2. Ordinal (23)\n",
    "\n",
    "Total no. of Categorical features = 23+23 = 46   \n",
    "Total no. of Numerical features = 14 + 20 = 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8a0982b1-a59d-402b-86b0-fe79f9bce06a",
    "_uuid": "8844a4f28f1df84b12d9fa01fbbf7901cedc9c88"
   },
   "source": [
    "## 1.1. Separate the numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5452a5df-cebb-40c9-b057-bab0d0acad32",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "ecce24be0abe9a2f23fd4ec56875a0740da7a11b"
   },
   "outputs": [],
   "source": [
    "# Separate out all the numeric features in the data\n",
    "num_columns = df_train._get_numeric_data().columns\n",
    "print(num_columns)\n",
    "print(len(num_columns))\n",
    "\n",
    "# Separate out all the non-numeric features in the data\n",
    "categ_columns = pd.Index(list(set(df_train.columns) - set(num_columns)))\n",
    "print(categ_columns)\n",
    "print(len(categ_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79afb297-fe7d-4aab-91d1-2ae598a84d5e",
    "_uuid": "0dc78474681f454a0f31f27f15f64e2397e6cc12"
   },
   "source": [
    "We notice that MSSubClass, OverallQual and OverallCond take numeric values but are actually categorical in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "76e17ba2-026c-4d2b-994c-faf80222ce1f",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "8f15448b4b6bf61f2287c6871d42f3ed8112e7a5"
   },
   "outputs": [],
   "source": [
    "# moving features from num_columns to categ_columns\n",
    "categ_columns = categ_columns.append(pd.Index(['MSSubClass', 'OverallQual', 'OverallCond']))\n",
    "num_columns = num_columns.drop(['MSSubClass', 'OverallQual', 'OverallCond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "691a41889d9433001515d326aa94e960136e5afe"
   },
   "outputs": [],
   "source": [
    "print(len(num_columns), len(categ_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aabdd064-056a-47e0-954f-159d7857fa61",
    "_uuid": "e8938b7a03c5498a604785c29325185f22063ced"
   },
   "source": [
    "## 1.2. Numerical features - *up close*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "57d9a8c8-145e-4c7f-8680-0448423e8e59",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "e7c3beacb6665e74e9b6a6acfe1de4106fc643bb"
   },
   "outputs": [],
   "source": [
    "df_train[num_columns].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f5df2d51-b6c3-4f20-9f9c-de9f9a45d150",
    "_uuid": "13be8253d88bf7050685518ef39bc0688a3effe5"
   },
   "source": [
    "### 1.2.1. Separating out the continuous and discrete features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3e152e2f5eae2106176b880c4e04b16ce972da9"
   },
   "source": [
    "Lets look at the number of unique values that each numeric feature takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f6255716-6126-44d0-aa98-de80c2416a66",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "0723b6d05d38948602e0c000077f42936684e7a3"
   },
   "outputs": [],
   "source": [
    "def barplot_with_anotate(feature_list, y_values):\n",
    "    x_pos = np.arange(len(feature_list))\n",
    "\n",
    "    plt.bar(x_pos, y_values);\n",
    "    plt.xticks(x_pos, feature_list, rotation=270);\n",
    "    for i in range(len(feature_list)):\n",
    "        plt.text(x=x_pos[i]-0.3, y=y_values[i]+1.0, s=y_values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ceb4d9525aa2a1c016504070d2518e0036609201"
   },
   "outputs": [],
   "source": [
    "feature_lengths_sorted = sorted([len(df_train[feature].unique()) for feature in num_columns])\n",
    "barplot_with_anotate(num_columns, feature_lengths_sorted)\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1aa80754-938e-4220-9927-484bccbe4c40",
    "_uuid": "ac0173d0a079ad875ad1daffbd35c9bdfd42bc4b"
   },
   "source": [
    "After eyeballing the previous output, we can observe that all the features that take on less than 30 unique values throughout the dataset are discrete. Let's separate them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e638aed5-de68-4927-805f-ae9543962b39",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "b9a868286ec2a7615a44f3ddfaf2ea5e68ad7fe6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_discrete_columns = []\n",
    "\n",
    "for feature in num_columns:\n",
    "    feature_len = len(df_train[feature].unique())\n",
    "    if feature_len < 30:\n",
    "        num_discrete_columns.append(feature)\n",
    "num_discrete_columns = pd.Index(num_discrete_columns)\n",
    "num_cont_columns = pd.Index(list(set(num_columns) - set(num_discrete_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "906b4a77-6e3c-4b50-b2d3-cf44685a9959",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "2b42b80160b1ed35b3989ec99b7b2ed3efee6a8e"
   },
   "outputs": [],
   "source": [
    "# print the details of those discrete valued features\n",
    "for feature in num_discrete_columns:\n",
    "    feature_len = len(df_train[feature].unique())\n",
    "    print(feature, feature_len)\n",
    "    print(df_train[feature].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a803516c-dc7b-4777-8860-9c64dd78a84f",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "1d07ad16c90380555fea3477502421f677649972"
   },
   "outputs": [],
   "source": [
    "# The features LowQualFinSF, 3SsnPorch, PoolArea, MiscVal \n",
    "# belong to the list of continous features as they take on values from a continous distribution.\n",
    "num_cont_columns = num_cont_columns.append(pd.Index(['LowQualFinSF', '3SsnPorch', 'PoolArea', 'MiscVal']))\n",
    "num_discrete_columns = num_discrete_columns.drop(['LowQualFinSF', '3SsnPorch', 'PoolArea', 'MiscVal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "1a786236f8e5935807f2eddd90744a22c8fd97e5"
   },
   "outputs": [],
   "source": [
    "# print the details of the continuous valued features\n",
    "for feature in sorted(num_cont_columns, key=lambda feature: len(df_train[feature].unique()), reverse=True):\n",
    "    feature_len = len(df_train[feature].unique())\n",
    "    print(feature, feature_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "c4b3c908cb3d85cf3ea20dd57ee3d4387b8f7734"
   },
   "outputs": [],
   "source": [
    "# The 3 year related features - YearBuilt, GarageYrBlt and YearRemodAdd belong to the list of discrete features.\n",
    "num_discrete_columns = num_discrete_columns.append(pd.Index(['YearBuilt', 'GarageYrBlt', 'YearRemodAdd']))\n",
    "num_cont_columns = num_cont_columns.drop(['YearBuilt', 'GarageYrBlt', 'YearRemodAdd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c22f46ef-8a34-4009-8a61-c079e32f954a",
    "_uuid": "589c68e5b567061eee19ed466eadb0152a9f32ad"
   },
   "source": [
    "## 1.3. Categorical features - *up close*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0501209d-7e8d-4d59-bb43-c8326f62faa7",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "cc84614e14b5818ccee1f6446e008da1050d37f3"
   },
   "outputs": [],
   "source": [
    "df_train[categ_columns].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4d50837f-ad53-4e93-b063-1e4d1f9f5110",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "7f188e25b7b8e13efd4d9d0140937a1732d1768c"
   },
   "outputs": [],
   "source": [
    "# lets look at what the categorical features represent\n",
    "for feature in sorted(categ_columns, key=lambda feature: len(df_train[feature].unique()), reverse=True):\n",
    "    feature_len = len(df_train[feature].unique())\n",
    "    print(feature, feature_len)\n",
    "    print(df_train[feature].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "29ba860c-eeae-453c-b24a-caa62b2e4212",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "ea6e9c78bbd93c58444864eddd60ab991f09029b"
   },
   "outputs": [],
   "source": [
    "# remove duplicate columns\n",
    "num_cont_columns = num_cont_columns.drop_duplicates()\n",
    "num_discrete_columns = num_discrete_columns.drop_duplicates()\n",
    "categ_columns = categ_columns.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7acc0e36-c4a5-4b3d-8f8b-301e79e0f13d",
    "_uuid": "d194e9783362a202ae97ce94d4e93f7239acb3d3"
   },
   "source": [
    "## 1.4. Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "48b019a4-0dbd-4b20-88a6-e52fcfb529b4",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "5cdd26f5e96875e3e8d98e5b7780b87062c52360"
   },
   "outputs": [],
   "source": [
    "print(\"Categorical columns: \", len(categ_columns))\n",
    "print(\"Continuous-valued numeric columns: \", len(num_cont_columns))\n",
    "print(\"Discrete-valued numeric columns: \", len(num_discrete_columns))\n",
    "print(\"-\"*10)\n",
    "print(\"Total columns: \", df_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "13c8eaf1-8737-4fa0-8eb0-c331813134ec",
    "_uuid": "56e77bae81d6c2f621f4d3ea0eb3ce680defbe9d"
   },
   "source": [
    "# 2. Preprocessing:\n",
    "We handle the separate kinds of features in this order:  \n",
    "* First, the continuous valued numeric features\n",
    "* Second, discrete valued numeric features\n",
    "* Third, categorical features, i.e, both nominal and ordinal features together\n",
    "\n",
    "But first, the author of the dataset recommends that we drop the data points with GrLivArea > 4000 as these are outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "98b21bc4-1c88-47e9-ab6e-a87f9c852ddb",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "4eb52983026b3cfd5980fab9e1987a84bf29f08c"
   },
   "outputs": [],
   "source": [
    "df_train.drop(df_train[df_train[\"GrLivArea\"] > 4000].index, inplace=True)\n",
    "df_train.shape\n",
    "\n",
    "ntrain = df_train.shape[0]\n",
    "ntest = df_test.shape[0]\n",
    "\n",
    "all_data = pd.concat((df_train, df_test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8fb6d686-dd11-48db-8f85-4754316d41cd",
    "_uuid": "e50e701679306c98a5e7c64a131fd7df8701a2da"
   },
   "source": [
    "## 2.1. Continuous valued numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "872ee0df-7a74-44dc-b917-59593d0f648e",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "b4001b795de12793613195d9dd70c4185e7472d0"
   },
   "outputs": [],
   "source": [
    "df_train[num_cont_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8d709c2cbd10c52bc40a45a7fa8b25b7eece311"
   },
   "source": [
    "### Fill in the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "8f63c14f54e8776452f8bea9efa8a1b5b6941c61"
   },
   "outputs": [],
   "source": [
    "def missing_features(data, column_set):\n",
    "    incomplete_features = {feature: data.shape[0]-sum(data[feature].value_counts())\n",
    "                                   for feature in column_set\n",
    "                                   if not sum(data[feature].value_counts()) == data.shape[0]}\n",
    "    incomplete_features_sorted = sorted(incomplete_features, key=lambda feature: incomplete_features[feature], reverse=True)\n",
    "    incompleteness = [round((incomplete_features[feature]/data.shape[0])*100, 2) for feature in incomplete_features_sorted]\n",
    "    barplot_with_anotate(incomplete_features_sorted, incompleteness)\n",
    "    plt.ylabel(\"Percentage (%) of values that are missing\")\n",
    "    plt.rcParams[\"figure.figsize\"] = [16, 8]\n",
    "    \n",
    "    for feature, percentage in zip(incomplete_features_sorted, incompleteness):\n",
    "        print(feature, incomplete_features[feature], \"(\", percentage, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "cce0658e461340ccf26e0979416e3b934d700ae9"
   },
   "outputs": [],
   "source": [
    "missing_features(all_data, num_cont_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4cdd01cd4c12161616abb1c140f47042b42b5ed"
   },
   "source": [
    "No columns are so empty that we need to drop them (I choose a 50% mark). Here's how we fill missing values:\n",
    "\n",
    "* LotFrontage - *mean of the column*\n",
    "* MasVnrArea, GarageArea, TotalBsmtSF, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF - \n",
    "    *0 (NA probably means that the features is absent)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1234ee66-e9a9-42a0-9ffa-621aa92f24ea",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "f7219eb76d5042dc7b281ebcd9efc66afa65add7"
   },
   "outputs": [],
   "source": [
    "all_data.loc[:, 'LotFrontage'].fillna(all_data['LotFrontage'].mean(), inplace=True)\n",
    "\n",
    "for feature in ['GarageArea', 'MasVnrArea', 'TotalBsmtSF', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF']:\n",
    "    all_data.loc[:, feature].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3261c0d9-da33-480f-aecc-ebb1822ffb24",
    "_uuid": "bfc46e4f9b9b1b2118e3116fde9701657c336770"
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "464ed5ea-f6bf-4f92-a653-5e4b520a1371",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "fd0db1931a297fbfcc52d9793fdb87514c93bb76"
   },
   "outputs": [],
   "source": [
    "# first we view a scatter plot of each feature vs. SalePrice\n",
    "\n",
    "num_cont_columns_list = list(num_cont_columns)\n",
    "\n",
    "# show max of 6 features in each row\n",
    "max_in_row = 6\n",
    "\n",
    "print(len(num_cont_columns_list))\n",
    "for i in range(0, len(num_cont_columns_list), max_in_row):\n",
    "    sns.pairplot(df_train, x_vars=num_cont_columns_list[i:i+max_in_row], y_vars=['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2311b545-73f9-4db7-b22e-e914c60fadfa",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "52b850ba09ab3bfbef33e994a3856a214943cd14"
   },
   "outputs": [],
   "source": [
    "# then we see the correlations\n",
    "print('Correlation of each feature with SalePrice:')\n",
    "corr = df_train[num_cont_columns].corr()\n",
    "corr_sorted = corr.sort_values([\"SalePrice\"], ascending = False)\n",
    "print(corr_sorted['SalePrice'])\n",
    "\n",
    "sns.set(font_scale=1.10)\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd93c397-bb7f-4f05-a944-a6c2a2d262b1",
    "_uuid": "3982eff76674d30aaf461f40c2b3aefbbc13c9cb"
   },
   "source": [
    "Upon eyballing the graphs and looking at the correlation table, it seems that LowQualFinSF, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, EnclosedPorch, BsmtFinSF2, LotFrontage, 2ndFlrSF by themselves don't have much correlation with the SalePrice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7bbf4131-6f5a-4c67-a489-59890e00a23e",
    "_uuid": "44a58ee00eb67a9a8dab06f6555450e0c638d727"
   },
   "source": [
    "__`OpenPorchSF`__ + __`EnclosedPorchSF`__ + __`3SsnPorch`__ + __`ScreenPorch`__\n",
    "\n",
    "There are 4 \"porch-area\" related features - OpenPorchSF, EnclosedPorchSF, 3SsnPorch, ScreenPorch.   \n",
    "Of these EnclosedPorch and 3SsnPorch have a small negative regression value while OpenPorchSF and ScreenPorch have small positive values.\n",
    "\n",
    "Lets **combine them all and form a new feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ba51b66-c5fb-41bb-ac7a-78a7bc89c784",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "7ff5f398808f24971926f78f6a759fa76a1b33cb"
   },
   "outputs": [],
   "source": [
    "all_data['TotPorchSF'] = all_data['OpenPorchSF'] + all_data['ScreenPorch'] + \\\n",
    "                         all_data['3SsnPorch'] + all_data['EnclosedPorch']\n",
    "num_cont_columns = num_cont_columns.append(pd.Index(['TotPorchSF']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c0abb0e-dd83-4da1-a091-9bb94c98dc72",
    "_uuid": "97a20e9ef2ea46a87d82f9812de49ae16f9cb61d"
   },
   "source": [
    "__`PoolArea`__\n",
    "\n",
    "PoolArea looks like it is mostly zero.\n",
    "\n",
    "Lets have a look.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d73bd146-c3b6-4548-a26f-fa7843fa9281",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "20d15b23194c9ae3b11f722864d0279d5bf564bd"
   },
   "outputs": [],
   "source": [
    "(df_train['PoolArea'] == 0).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "195dbb02-e980-4be4-a716-3d56bd5df215",
    "_uuid": "46cd358ab95e378e061795e467ec3a6dc08c33d7"
   },
   "source": [
    "It is highly skewed: the ratio of non-zero to zeros is less than 1/200. We're better off **dropping PoolArea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bd907b39-5edf-4110-9b37-d19e77970c13",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "cae63c53451fe85bad23274be079b1497b690c5b"
   },
   "outputs": [],
   "source": [
    "all_data.drop(columns=['PoolArea'], inplace=True)\n",
    "num_cont_columns = num_cont_columns.drop(['PoolArea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc267f0b-c5b8-45cc-a7e4-b3a3802f3dbd",
    "_uuid": "3e8f947ecb7e39f9fbd3bb8b8dd6efb758c5cf7a"
   },
   "source": [
    "__`TotalBsmtSF + 1stFlrSF + 2ndFlrSF`__\n",
    "\n",
    "Define a new feature - `TotalSF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc282f60-a616-4a42-b46d-16d5b1ccf243",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "06c50c6c11730e98fd92737d2f69e8a2b65b9f3c"
   },
   "outputs": [],
   "source": [
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "num_cont_columns = num_cont_columns.append(pd.Index(['TotalSF']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c1b89c1-e231-4e6c-97b9-c314cd8bd7da",
    "_uuid": "43f93a0101dd84bfff186dbec3e55cd6ebfb10c1"
   },
   "source": [
    "And drop `TotalBsmtSF`, `1stFlrSF` and `2ndFlrSF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c643a1e4-dd2a-4cca-99c1-425ac6873d5b",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "ca9879d099888bb242835753d4cc11a4a2a86bb2"
   },
   "outputs": [],
   "source": [
    "all_data.drop(columns=['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], inplace=True)\n",
    "num_cont_columns = num_cont_columns.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9058fdfefb412d041a556b01e2eb5142cd77248"
   },
   "source": [
    "**Log-transform of `SalePrice`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79707d568cdfd3cec3b6dfa6ec8f671511fd333a"
   },
   "outputs": [],
   "source": [
    "df_train = all_data[:ntrain][:]\n",
    "sns.distplot(df_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b40f91b81b64f24b847c0bee72bcd19b75fa636"
   },
   "outputs": [],
   "source": [
    "'''SalePriceLog = np.log1p(df_train['SalePrice'])\n",
    "sns.distplot(SalePriceLog)'''\n",
    "SalePriceLog = df_train['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de191df1-2ea5-4fd8-96e7-c2566b8ec74e",
    "_uuid": "e5247c3726b77599e1be001928d094c6826c7f51"
   },
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da628c22-a721-442f-9f0c-5dba518fb7c2",
    "_uuid": "cbcec5254eece8d8a257da095160d9951c14415b",
    "collapsed": true
   },
   "source": [
    "## 2.2. Discrete valued numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "05fd3e54-9bb5-454e-bf0d-f34aac9b9799",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "5521eed2f44608ab2807a06e20b0a5e34c64e958",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train[num_discrete_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8ba9c2c1-5ce6-4ca6-a588-970d2ec44fb4",
    "_uuid": "01bbcb53e2a2e410910e146ba5fffff99dfa0da3"
   },
   "source": [
    "### Filling in missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "fb8668db7c977cc6ff7a1dc7a15d8ab163f91d9a"
   },
   "outputs": [],
   "source": [
    "missing_features(all_data, num_discrete_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "758985819a9f3e3237bc827933821f44c9a009cc"
   },
   "source": [
    "* `GarageYrBlt`: We fill the missing values with the corresponding `YearBuilt` values (assume that the garage was built along with the house)\n",
    "* `BsmtFullBath`: 0 (NaN probably means not present)\n",
    "* `BsmtHalfBath`: 0 (NaN probably means not present)\n",
    "* `GarageCars`: 0 (NaN probably means not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "bd6b1797993b58b96231b325c94b7edf0125c554"
   },
   "outputs": [],
   "source": [
    "all_data.loc[:, 'GarageYrBlt'].fillna(all_data['YearBuilt'], inplace=True)\n",
    "all_data.loc[:, 'BsmtFullBath'].fillna(0, inplace=True)\n",
    "all_data.loc[:, 'BsmtHalfBath'].fillna(0, inplace=True)\n",
    "all_data.loc[:, 'GarageCars'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "52f4cd0d-2a7b-421a-85d0-5b9b2efe1ffe",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "9d77c1b149cfe141e71f3718789cd7edfd3acada"
   },
   "outputs": [],
   "source": [
    "# print the details of those discrete valued features\n",
    "for feature in num_discrete_columns:\n",
    "    feature_len = len(df_train[feature].unique())\n",
    "    print(feature, feature_len)\n",
    "    print(df_train[feature].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a0663671-4770-47f6-bc46-93f29bd299eb",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "573770d8838f5aad0449e23bb310230dc0bc4b7a"
   },
   "outputs": [],
   "source": [
    "print('Total: ', len(num_discrete_columns))\n",
    "corr = df_train[list(num_discrete_columns) + ['SalePrice']].corr()\n",
    "corr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\n",
    "print(corr['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e5e0f72-96c8-45e9-a631-7220e162562c",
    "_uuid": "3e0e445a82fb4f40165636a4e37df3e7cb24ee03"
   },
   "source": [
    "__`GarageArea`__:\n",
    "\n",
    "Notice that GarageCars in this discrete variables list and GarageArea in continuous variables list, both convey the garage capacity. Since GarageCars has a higher correlation, we **drop GarageArea.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "fd1bb4375e0e82347cb382a6739b90a36387d5fc"
   },
   "outputs": [],
   "source": [
    "all_data.drop(columns=['GarageArea'], inplace=True)\n",
    "num_cont_columns = num_cont_columns.drop(['GarageArea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8469fa87-9083-4116-9f29-02d2c61a36f0",
    "_uuid": "933a20e0d343f6fc1990a3db970c6c34ef41c367"
   },
   "source": [
    "#### We change the date related features\n",
    "\n",
    "It would be helpful if we could map the features that represent some date (like say, *1888*) to some small number (like say, *88*). \n",
    "There are 3 year related features - YearBuilt, YearRemodAdd, GarageYrBlt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ac180536-3e98-4ffd-8de5-2127c247a543",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "0c327c0a9362bf05197955ba0054f132721b2a95"
   },
   "outputs": [],
   "source": [
    "df_train[['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "55c10cbf-9334-47d1-9f96-88c40dfe16da",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "d7f0928ed6b7e74dbf9d540151d9ac8b0aedc6d6"
   },
   "outputs": [],
   "source": [
    "# Since the min of all 3 is 1872, we could replace their values with something like `year % 1872`\n",
    "\n",
    "all_data['YearBuilt'] = all_data['YearBuilt'] % 1872\n",
    "all_data['YearRemodAdd'] = all_data['YearRemodAdd'] % 1872\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'] % 1872"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "539f159e85ecfa806b93926cc0624aff4a199c01"
   },
   "source": [
    "__`Bath`__ = __`FullBath`__ + 0.5*__`HalfBath`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0658673a27d54e7ba9a89bd8fcddaa18121ed5c"
   },
   "outputs": [],
   "source": [
    "all_data['Bath'] = all_data['FullBath'] + 0.5*all_data['HalfBath']\n",
    "all_data.drop(columns=['FullBath', 'HalfBath'], inplace=True)\n",
    "num_discrete_columns = num_discrete_columns.drop(['FullBath', 'HalfBath'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62270be5594d32ddbd32cc303c9b86193eafb120"
   },
   "source": [
    "__`BsmtBath`__ = __`BsmtFullBath`__ + 0.5*__`BsmtHalfBath`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2979199949be69a7dd6967b835354e7900bde53d"
   },
   "outputs": [],
   "source": [
    "all_data['BsmtBath'] = all_data['BsmtFullBath'] + 0.5*all_data['BsmtHalfBath']\n",
    "all_data.drop(columns=['BsmtFullBath', 'BsmtHalfBath'], inplace=True)\n",
    "num_discrete_columns = num_discrete_columns.drop(['BsmtFullBath', 'BsmtHalfBath'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d8597c10-8dd3-4cee-b181-fde4f152622f",
    "_uuid": "ead4e179c36874cc619207c119edbefcb79010e2"
   },
   "source": [
    "## 2.3. Categorical features:\n",
    "### Preprocessing\n",
    "#### Filling out the missing values -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7fe07c35-e8f7-4020-b82e-6fe2601821b6",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "ebff1fd6ee7309d0800ae5d5a6e9b0271cd7244c"
   },
   "outputs": [],
   "source": [
    "missing_features(all_data, categ_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82a7f0d2-c168-42c9-a2da-419f2cfcd684",
    "_uuid": "dd6bca863eb33c558a59e7234065f8fe8b0a6aea"
   },
   "source": [
    " The features are more than 50% empty - `PoolQC`, `MiscFeature`, `Alley`, `Fence`. Let's see their value counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c6c14b34-9d3b-4d3a-aae5-4fca48cd5723",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "b37340a817f00d978a24e2d54da895a6e6421b19"
   },
   "outputs": [],
   "source": [
    "for feature in ['PoolQC', 'MiscFeature', 'Alley', 'Fence']:\n",
    "    print(df_train[feature].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a3479f77-cfe7-4a11-8d95-4474e836ae8b",
    "_uuid": "48946b938ebad63fc90c39d239c42ebbce6a00ac"
   },
   "source": [
    "Points to note:\n",
    "* We should drop `PoolQC` as it is mostly empty and because we have already dropped `PoolArea`\n",
    "* The [feature description file](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt) says that `MiscFeature`, `Alley` and `Fence` have a value `NA` to represent the absence of those features. Yet, no datapoint has that value. This probably means that during the collecting of data, the person left this column blank instead of filling `NA`. So, we should take the empty values to mean `NA`\n",
    "* `MiscFeature` is mostly just used to say the presence of Shed. So, we could replace it with a feature - `Shed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "63b44530-a3cb-48a4-bf7f-76ef889dfb7c",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "0ea1242f482cbfaf683ff6112ffed6bd8d95fa9b"
   },
   "outputs": [],
   "source": [
    "# drop PoolQC\n",
    "all_data.drop(columns=['PoolQC'], inplace=True)\n",
    "categ_columns = categ_columns.drop(['PoolQC'])\n",
    "\n",
    "# filling NA\n",
    "all_data.fillna(value= {'MiscFeature': 'NA',\n",
    "                        'Fence': 'NA',\n",
    "                        'Alley': 'NA'}, inplace=True)\n",
    "\n",
    "# Shed\n",
    "all_data['Shed'] = all_data['MiscFeature'] == 'Shed'\n",
    "all_data.drop(columns=['MiscFeature'], inplace=True)\n",
    "categ_columns = categ_columns.drop(['MiscFeature'])\n",
    "categ_columns = categ_columns.append(pd.Index(['Shed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "5392c52d1597515e208437ce3e2ab7c90a05a67e"
   },
   "outputs": [],
   "source": [
    "filling_dict = {'FireplaceQu': 'NA',\n",
    "                'GarageFinish': 'NA',\n",
    "                'GarageQual': 'NA',\n",
    "                'GarageType': 'NA',\n",
    "                'GarageCond': 'NA',\n",
    "                'BsmtExposure': 'NA',\n",
    "                'BsmtFinType2': 'NA',\n",
    "                'BsmtFinType1': 'NA',\n",
    "                'BsmtCond': 'NA',\n",
    "                'BsmtQual': 'NA',\n",
    "                'MasVnrType': 'None',\n",
    "                'Exterior1st': 'Other',\n",
    "                'Exterior2nd': 'Other',\n",
    "                'SaleType': 'Oth'}\n",
    "\n",
    "for feature in ['Electrical', 'MSZoning', 'Functional', 'Utilities', 'KitchenQual']:\n",
    "    filling_dict[feature] = all_data[feature].mode().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ff27262f-cd9a-4030-b737-58e5bed6cf82",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "51f9bb52dff5a0499a62488745ecd838f5d2fc73"
   },
   "outputs": [],
   "source": [
    "# Now, handle the rest of the incomplete features\n",
    "all_data.fillna(value=filling_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ca3ccca-41ba-48e8-a942-97e490be216b",
    "_uuid": "fa645229872504a78265e74a2b554d100fadc6e3"
   },
   "source": [
    "#### Dropping features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0348133b-456b-461b-9a54-804a0042eaed",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "3d4c737c8545f700383091268e4c990e6b922ff4"
   },
   "outputs": [],
   "source": [
    "# Neighborhood is an ordinal feature but we can't decide on the order responsibly unless we know the Ames city\n",
    "# Hence we drop it\n",
    "all_data.drop(columns=['Neighborhood'], inplace=True)\n",
    "categ_columns = categ_columns.drop(['Neighborhood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79891612-11c5-489b-9183-819e060752a8",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "8660541a54aa356f1bd4be2188185c2eff386ea7"
   },
   "outputs": [],
   "source": [
    "# GarageQual and GarageCond measure the same thing\n",
    "all_data.drop(columns=['GarageQual'], inplace=True)\n",
    "categ_columns = categ_columns.drop(['GarageQual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ee9ec1a-8f81-4d22-824e-a451c24ca00d",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "a5192642fe0205ed962a7db9298f86b46e8ec178"
   },
   "outputs": [],
   "source": [
    "# OverallQual and OverallCond seem to measure the same thing\n",
    "# yet they have a very different correlation coefficient with SalePrice\n",
    "df_train[['SalePrice', 'OverallQual', 'OverallCond']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "f33f65a77fd283204d10b84e25917bc8b0cffa04"
   },
   "outputs": [],
   "source": [
    "# drop OverallCond\n",
    "all_data.drop(columns=['OverallCond'], inplace=True)\n",
    "categ_columns = categ_columns.drop(['OverallCond'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3eb5d85c-ad00-4d4f-8aa7-a48bd630b24b",
    "_uuid": "60cd3e13babcbf7a614cea7c5f4c77cec8012119"
   },
   "source": [
    "#### More preprocessing -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a432fb84-a3aa-4acb-8524-6b8cd92c0991",
    "_uuid": "552593e902c97ee2eaa7cfba2cbc4e332e3ed924"
   },
   "source": [
    "#### `MSSubClass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ec4ea21f-4a12-49f8-9e9e-e61d6a7bb7ce",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "b9c9f0fd63274dec8d13d8275fe3200e689e14d9"
   },
   "outputs": [],
   "source": [
    "# new feature extraction\n",
    "all_data[\"NewerDwelling\"] = all_data[\"MSSubClass\"].isin([20, 60, 120, 160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f86709a9-5b7e-476e-a46e-4a25111ebba3",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "439492bee9e74434865cb9b0100f842ac2950ebb"
   },
   "outputs": [],
   "source": [
    "# NewerDwelling, BldgType and HouseStyle together capture everything that MSSubClass can tell\n",
    "# Hence we drop MSSubClass\n",
    "all_data.drop(columns=['MSSubClass'], inplace=True)\n",
    "categ_columns = categ_columns.drop(['MSSubClass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af777400-f972-436d-9418-4f3c0b026c96",
    "_uuid": "7eec10ad8bb4e3b3840dac740d2296d927e49bbc"
   },
   "source": [
    "#### `MSZoning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2e610d4a-a97e-4c77-bfad-e029bd1632c4",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "f614a95ebd2e582fcf5c47f2081546742b6f8574"
   },
   "outputs": [],
   "source": [
    "# simplity MSZoning as Residential or Non-residential\n",
    "all_data['Residential'] = all_data['MSZoning'].isin(['RH', 'RL', 'RP', 'RM'])\n",
    "\n",
    "# and drop MSZoning\n",
    "all_data.drop(columns=['MSZoning'], inplace=True)\n",
    "categ_columns = categ_columns.drop(['MSZoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "405d102f-68fe-47a4-85bb-f4a8c15e680d",
    "_uuid": "f997a9ff3fb68aa672412b583ab62e3258a1df4f"
   },
   "source": [
    "#### `Exterior1st` and `Exterior2nd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "053d108e-11a6-4e55-aa74-78e15a37fa0d",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "1b0bc29a151942e08888db7b6ab86fab1b122e93"
   },
   "outputs": [],
   "source": [
    "# simplify Exterior1st and Exterior2nd \n",
    "print(df_train['Exterior1st'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6d453602-37f7-4a8d-976b-a8fb62a8add9",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "dc8b5ccf91c4891220fb94a2caea0d5710afb4b6"
   },
   "outputs": [],
   "source": [
    "# We keep only the top 4 and move the rest to `Other`\n",
    "all_data['Exterior1st'][-all_data['Exterior1st'].isin(all_data['Exterior1st'].value_counts().index[0:4])] \\\n",
    "    = 'Other'\n",
    "all_data['Exterior2nd'][-all_data['Exterior2nd'].isin(all_data['Exterior1st'].value_counts().index[0:4])] \\\n",
    "    = 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "237b2386-1894-4395-a050-6868be1e6055",
    "_uuid": "e13ae76d3c42db1480fcd99c53c750d974bfc377"
   },
   "source": [
    "#### `SaleType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7f0a5b34-0166-4a63-952c-bd3a14a11935",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "eeef8161f18adc33da4d49edd9126d5b2cdcac4a"
   },
   "outputs": [],
   "source": [
    "# simplify SaleType\n",
    "all_data['SaleType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc7d4284-cef5-414b-852a-6558ca9265a3",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "a4e96de04ffef3fd1beced52607ab1c8e5c6011d"
   },
   "outputs": [],
   "source": [
    "# keep only the top 3 and move rest to `Oth`\n",
    "all_data['SaleType'][-all_data['SaleType'].isin(all_data['SaleType'].value_counts().index[0:3])] = 'Oth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "83e0c2f9-b4c4-4e2c-8a87-ea328897b0c9",
    "_uuid": "e6ae5e4cc4f2c6300823674d45a7f3ef85f3548d"
   },
   "source": [
    "#### `Functional`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "edcd06ac-d1c3-42f7-b1b1-66e36dcecfbc",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "d27915db85cd0cec3771013e60dd1970081d409e"
   },
   "outputs": [],
   "source": [
    "# simplify Functional\n",
    "all_data['Functional'][all_data['Functional'].isin(['Min1', 'Min2'])] = 'Min'\n",
    "all_data['Functional'][all_data['Functional'].isin(['Maj1', 'Maj2'])] = 'Maj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5da7b49d-f0a7-4428-9276-17d87be91954",
    "_uuid": "ed0fe56210932cc8c1e61655afe9fde67e32e463"
   },
   "source": [
    "#### `GarageType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b1c86fe-f7f1-456e-9740-e6e86eb90848",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "8a68a04fb3e5994bc94460ea393e64edf18c6c6a"
   },
   "outputs": [],
   "source": [
    "# GarageType can either be Attchd or Detchd\n",
    "# new feature - GarageDetchd\n",
    "all_data['GarageDetchd'] = all_data['GarageType'].replace({'BuiltIn': 'Attchd', \n",
    "    'Basment': 'Attchd', 'CarPort': 'Attchd', '2Types': 'Detchd'})\n",
    "\n",
    "# drop GarageType\n",
    "all_data.drop(columns=['GarageType'], inplace=True)\n",
    "categ_columns = categ_columns.drop(['GarageType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8698175c-9f7f-46ab-97f9-4a2032f6a532",
    "_uuid": "f67e767f0d70b236e9b3b558db58fdaf15574021"
   },
   "source": [
    "#### Transforming categorical features\n",
    "* Ordered numbering - for ordinal features\n",
    "* Some other numbering format - for nominal features\n",
    "\n",
    "This way we help to prevent in embedding any false assumptions into our data because\n",
    "* With ordered features there is a sense of structure and order in the values that those features take (for eg. - Excellent > Good > Poor)\n",
    "* With nominal data if we use some order in the numbering, we embed false structure to our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2cfc22c4-a5e5-4151-bead-0e7d5c9cbdaa",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "3685b185f1523c113be04c725e80abfa4ff8b443"
   },
   "outputs": [],
   "source": [
    "# Step 1: Convert ordinal features to numbers\n",
    "replace_dict =   {'LotShape': {'IR3': 0, 'IR2': 1, 'IR1': 2, 'Reg': 3},\n",
    "                  'Utilities': {'ELO': 0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n",
    "                  'LandSlope': {'Sev': 0, 'Mod': 1, 'Gtl': 2},\n",
    "                  'ExterQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
    "                  'ExterCond': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
    "                  'BsmtQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "                  'BsmtCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "                  'BsmtExposure': {'NA': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "                  'BsmtFinType1': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "                  'BsmtFinType2': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "                  'HeatingQC': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
    "                  'Electrical': {'Mix': 0, 'FuseP': 1, 'FuseF': 2, 'FuseA': 3, 'SBrkr': 4},\n",
    "                  'KitchenQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
    "                  'Functional': {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2': 5, 'Min1': 6,\n",
    "                                 'Typ': 7},\n",
    "                  'FireplaceQu': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "                  'GarageFinish': {'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n",
    "                  'GarageCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "                  'PavedDrive': {'N': 0, 'P': 1, 'Y': 2},\n",
    "                  'Fence': {'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4},\n",
    "                  'Condition1': {'Artery': 0, 'Feedr': 0, 'RRAn': 1, 'RRAe': 1, 'RRNn': 2, 'RRNe': 2, \n",
    "                                 'Norm': 5, 'PosA': 10, 'PosN': 11},\n",
    "                  'Condition2': {'Artery': 0, 'Feedr': 0, 'RRAn': 1, 'RRAe': 1, 'RRNn': 2, 'RRNe': 2, \n",
    "                                 'Norm': 5, 'PosA': 10, 'PosN': 11}}\n",
    "all_data = all_data.replace(replace_dict)\n",
    "\n",
    "# all the other features of categ_columns are nominal\n",
    "nominal_columns = categ_columns.drop(list(replace_dict.keys()))\n",
    "nominal_columns = nominal_columns.drop(['OverallQual'])\n",
    "ordinal_columns = pd.Series(list(replace_dict.keys()).append('OverallQual'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb203d79-b76f-4f83-afd0-332580549ee7",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "254450b9f1cf9e3b3bc45cd9e133ee10c5d3f1ef"
   },
   "outputs": [],
   "source": [
    "# Step 2: One-hot encoding for nominal features\n",
    "all_data = pd.get_dummies(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "cd6110243103f190bba3852f698ac1caf8087ada"
   },
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96e75933-9589-46ac-a2fe-c0cc4e6e5f3c",
    "_uuid": "2b421136e400eca8d87838d27f3d681d0b351287"
   },
   "source": [
    "## 2.4. Final feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ca8fc219e611b150cef1871d763e811bdbc7b4a"
   },
   "source": [
    "### Creating newer features by combining existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9bb9455a-e56d-413c-b6fc-61154d274ea5",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "480923b847bcf4cb8b302967a651c5fa20d12f84"
   },
   "outputs": [],
   "source": [
    "# BsmtFinType1 * BsmtFinSF1\n",
    "all_data['BsmtFin1_Type*SF'] = all_data['BsmtFinType1'] * all_data['BsmtFinSF1']\n",
    "\n",
    "# BsmtFinType2 * BsmtFinSF2\n",
    "all_data['BsmtFin2_Type*SF'] = all_data['BsmtFinType2'] * all_data['BsmtFinSF2']\n",
    "\n",
    "# ExterQual * ExterCond\n",
    "all_data['Exter_Qual*Cond'] = all_data['ExterQual'] * all_data['ExterCond']\n",
    "\n",
    "# KitchenQual * no. of kitchens\n",
    "all_data['Kitchen_no*Qual'] = all_data['KitchenAbvGr'] * all_data['KitchenQual']\n",
    "\n",
    "# Condition1 * Condition2\n",
    "all_data['Condition1*Contition2'] = all_data['Condition1'] * all_data['Condition2']\n",
    "\n",
    "# OverallQual * TotalSF\n",
    "all_data['OverallQual*TotalSF'] = all_data['OverallQual'] * all_data['TotalSF']\n",
    "\n",
    "# BsmtQual * BsmtCond * BsmtExposure\n",
    "all_data['BsmtQual*Cond*Expo'] = all_data['BsmtQual'] * all_data['BsmtCond'] * all_data['BsmtExposure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31d4804e1f711735dd183fe768f20f429636d3c2"
   },
   "source": [
    "### Adding polynomials of the top 10 most correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "252417b6787f271c246bc3ba993ba841d7e0ace7"
   },
   "outputs": [],
   "source": [
    "# get a list of the top 10 most correlated features\n",
    "corr = all_data[all_data.columns].corr()\n",
    "corr_sorted = corr.sort_values([\"SalePrice\"], ascending = False)\n",
    "top10_features = list(corr_sorted['SalePrice'][1:11].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8dd7ca6124f267b019ce86e84f3db1f9e67ea8b"
   },
   "outputs": [],
   "source": [
    "# generate polynomial features and add to existing DataFrame\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features.fit_transform(all_data[top10_features])\n",
    "poly_features.get_feature_names(top10_features)\n",
    "\n",
    "poly_df = pd.DataFrame(poly_features.transform(all_data[top10_features]), \n",
    "                                     columns=poly_features.get_feature_names(top10_features))\n",
    "\n",
    "all_data = pd.concat([all_data, poly_df], axis=1)\n",
    "\n",
    "print(\"Final shape of data: \", all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b591ec6322528b5cd3b90b72305aead91553de3e"
   },
   "source": [
    "This is what the new features look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "338d6dedd7ab0f52eb51a6442ee2966c4ee8daa5"
   },
   "outputs": [],
   "source": [
    "poly_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e1f8fa3ff2c11de03334bd84e701ffd3176d8f3"
   },
   "source": [
    "### Dropping uncorrelated features\n",
    "\n",
    "We drop the features which have a `|correlation score| < 0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a5b9940f2c9bb1a7fc54b85b2ccfd2d42460499"
   },
   "outputs": [],
   "source": [
    "# then we see the correlations\n",
    "corr = all_data[all_data.columns].corr()\n",
    "corr_sorted = corr.sort_values([\"SalePrice\"], ascending = False)\n",
    "\n",
    "#corr_sorted['SalePrice'][0:50]\n",
    "best_corr = corr_sorted[corr_sorted['SalePrice']>0.1]['SalePrice']\n",
    "worst_corr = corr_sorted[corr_sorted['SalePrice']<-0.1]['SalePrice']\n",
    "selected_corr = pd.concat([best_corr, worst_corr])\n",
    "\n",
    "print(len(list(selected_corr)))\n",
    "print(len(list(best_corr)), len(list(worst_corr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94824419c8ae71272ecb09626c3df79c32f8ca67"
   },
   "outputs": [],
   "source": [
    "black_list = [elem for elem in list(all_data.columns) if elem not in list(selected_corr.keys())]\n",
    "all_data.drop(columns=black_list, inplace=True)\n",
    "\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3583c669991be093210a39188855c4fdab313c1"
   },
   "source": [
    "### Handling multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f03ace8dc6a54359c142f5492b3cb1cc7607fdf0"
   },
   "outputs": [],
   "source": [
    "attrs = corr.drop('SalePrice').drop('SalePrice', axis=1)\n",
    "\n",
    "threshold = 0.8\n",
    "important_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]) \\\n",
    "    .unstack().dropna().to_dict()\n",
    "    \n",
    "print(\"There are\", len(important_corrs), \"pairs of non-target features with considerable correlations amongst them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "265ebba79a4d54b307ccac6f13b45316dc909213"
   },
   "source": [
    "I believe that the model will perform much better if I reduce the multicollinearity.\n",
    "\n",
    "Using PCA could do the job but it would also create uncomprehensible features. I need to use some better alternative. Please comment below if you have one in mind.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0ce10a2e-0c98-41dd-afec-7d86229629fc",
    "_uuid": "c6ab0590b4ccaf3edd48b801d1e27ce9c1285865",
    "collapsed": true
   },
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8437601d52a23fefb1ccf690460de94a36b4edc"
   },
   "outputs": [],
   "source": [
    "all_data = all_data.loc[:,~all_data.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fcff951c195cf849ef5231a8f9fd1552e66bec4"
   },
   "source": [
    "**Divide the train data into `df_train` and `df_validate`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "509797c0afc4eca9581a07d98e93fa88bc0c0d79"
   },
   "outputs": [],
   "source": [
    "nvalidate = int(0.2 * ntrain)\n",
    "ntrain = int(ntrain - nvalidate)\n",
    "\n",
    "df_train = all_data[:ntrain][:]\n",
    "df_validate = all_data[ntrain:ntrain+nvalidate][:]\n",
    "df_test = all_data[ntrain+nvalidate:][:]\n",
    "df_test = df_test.drop('SalePrice', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0f1c1add13f58b7d13b46429a615a999794bc47"
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_validate.shape)\n",
    "print(df_test.shape)\n",
    "print(ntrain, nvalidate, ntest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "641c88df267d64106d4364f102d56e8f38d04675"
   },
   "source": [
    "**Separate the true `SalePrice` column from the rest of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8eadcea8c386bb982ab369e516d4a4cb7d363de"
   },
   "outputs": [],
   "source": [
    "X_train= df_train.drop('SalePrice', axis= 1)\n",
    "Y_train= df_train['SalePrice']\n",
    "Y_train_log = SalePriceLog[:ntrain]\n",
    "\n",
    "X_validate = df_validate.drop('SalePrice', axis=1)\n",
    "Y_validate = df_validate['SalePrice']\n",
    "Y_validate_log = SalePriceLog[ntrain:ntrain+nvalidate]\n",
    "\n",
    "X_test= df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb651982db89689100212c70beb61619839a40ea"
   },
   "outputs": [],
   "source": [
    "# make sure that there are no NaNs\n",
    "missing_features(X_train, X_train.columns)\n",
    "missing_features(X_test, X_test.columns)\n",
    "missing_features(X_validate, X_validate.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8980406a928062a1575b1d7317f85325b161db07"
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc5299fb79d1fda0d8daeb36fbd02107d6c08137"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80678a45417bed1ab311335e3105c436e73865e3"
   },
   "outputs": [],
   "source": [
    "def rmse_score(Y_true, Y_pred):\n",
    "    return round(np.sqrt(mean_squared_error(np.log(Y_true), np.log(Y_pred))), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68a9e44fb35a9bef9bfc50fdd7cb029be0c8d9af"
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e952d438811485431e192dbc40aa28f8d6da0bd"
   },
   "outputs": [],
   "source": [
    "linreg = LinearRegression(normalize=True)\n",
    "linreg.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = linreg.predict(X_train)\n",
    "Y_train_pred[Y_train_pred < 1] = 1\n",
    "Y_validate_pred = linreg.predict(X_validate)\n",
    "Y_validate_pred[Y_validate_pred<1] = 1\n",
    "\n",
    "acc_lin_train = rmse_score(Y_train.values, Y_train_pred)\n",
    "acc_lin_validate = rmse_score(Y_validate.values, Y_validate_pred)\n",
    "\n",
    "print(\"RMSE on train: \", acc_lin_train)\n",
    "print(\"RMSE on validation: \", acc_lin_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a8dfc5e0fb402f03d218f5fab64b5f59b0057b2"
   },
   "outputs": [],
   "source": [
    "linreg_on_log = LinearRegression(normalize=True)\n",
    "linreg_on_log.fit(X_train, Y_train_log)\n",
    "\n",
    "Y_train_pred = np.exp(linreg_on_log.predict(X_train))\n",
    "Y_train_pred[Y_train_pred < 0] = 0\n",
    "Y_validate_pred = np.exp(linreg_on_log.predict(X_validate))\n",
    "Y_validate_pred[Y_validate_pred<0] = 0\n",
    "\n",
    "acc_lin_train = rmse_score(Y_train.values, Y_train_pred)\n",
    "acc_lin_validate = rmse_score(Y_validate.values, Y_validate_pred)\n",
    "\n",
    "print(\"RMSE on train: \", acc_lin_train)\n",
    "print(\"RMSE on validation: \", acc_lin_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c46b2075ca22e70a5b71ceefbaf7ae88643eb1d"
   },
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f427d0ded2670168f39027fb5db8a625f844d29e"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_train, Y_train)\n",
    "\n",
    "acc_lasso_train = rmse_score(Y_train.values, lasso.predict(X_train))\n",
    "acc_lasso_validate = rmse_score(Y_validate.values, lasso.predict(X_validate))\n",
    "\n",
    "print(\"RMSE on train: \", acc_lasso_train)\n",
    "print(\"RMSE on validation: \", acc_lasso_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c26c20d8e5f6b87ebc9014edbcc838c042e5030"
   },
   "outputs": [],
   "source": [
    "lasso_on_log = Lasso()\n",
    "lasso_on_log.fit(X_train, Y_train_log)\n",
    "\n",
    "acc_lasso_train = rmse_score(Y_train.values, np.exp(lasso_on_log.predict(X_train)))\n",
    "acc_lasso_validate = rmse_score(Y_validate.values, np.exp(lasso_on_log.predict(X_validate)))\n",
    "\n",
    "print(\"RMSE on train: \", acc_lasso_train)\n",
    "print(\"RMSE on validation: \", acc_lasso_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b737f0ccf39242b1869f9946576084d78efb872"
   },
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e86cff8e9fcc2bab869cc6005176faec201c5da"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X_train, Y_train)\n",
    "\n",
    "acc_ridge_train = rmse_score(Y_train.values, ridge.predict(X_train))\n",
    "acc_ridge_validate = rmse_score(Y_validate.values, ridge.predict(X_validate))\n",
    "\n",
    "print(\"RMSE on train: \", acc_ridge_train)\n",
    "print(\"RMSE on validation: \", acc_ridge_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e94ddea13c00e66643dca2dfa9e281591b7bf12"
   },
   "outputs": [],
   "source": [
    "ridge_on_log = Ridge()\n",
    "ridge_on_log.fit(X_train, Y_train_log)\n",
    "\n",
    "acc_ridge_train = rmse_score(Y_train.values, np.exp(ridge_on_log.predict(X_train)))\n",
    "acc_ridge_validate = rmse_score(Y_validate.values, np.exp(ridge_on_log.predict(X_validate)))\n",
    "\n",
    "print(\"RMSE on train: \", acc_ridge_train)\n",
    "print(\"RMSE on validation: \", acc_ridge_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8f4a6cca07ad3434c9557a232a08fcd8e7a8c58"
   },
   "source": [
    "### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85721b9709613181732078d9ede238f5a0a9906e"
   },
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet()\n",
    "elastic_net.fit(X_train, Y_train)\n",
    "\n",
    "acc_en_train = rmse_score(Y_train.values, elastic_net.predict(X_train))\n",
    "acc_en_validate = rmse_score(Y_validate.values, elastic_net.predict(X_validate))\n",
    "\n",
    "print(\"RMSE on train: \", acc_en_train)\n",
    "print(\"RMSE on validation: \", acc_en_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d4ae9da044b39ec8bf4d6d4b32a16ae86bf32601"
   },
   "outputs": [],
   "source": [
    "elastic_net_on_log = ElasticNet()\n",
    "elastic_net_on_log.fit(X_train, Y_train_log)\n",
    "\n",
    "acc_en_train = rmse_score(Y_train.values, np.exp(elastic_net_on_log.predict(X_train)))\n",
    "acc_en_validate = rmse_score(Y_validate.values, np.exp(elastic_net_on_log.predict(X_validate)))\n",
    "\n",
    "print(\"RMSE on train: \", acc_en_train)\n",
    "print(\"RMSE on validation: \", acc_en_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0dc205d3449a8f0b22da00ddb305203ac1c8f367"
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ffb2c4f60fa6e4cdc4fd2e687841b76c3726e20"
   },
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "\n",
    "acc_decision_tree_train = rmse_score(Y_train.values, decision_tree.predict(X_train))\n",
    "acc_decision_tree_validate = rmse_score(Y_validate.values, decision_tree.predict(X_validate))\n",
    "\n",
    "print(\"RMSE on train: \", acc_decision_tree_train)\n",
    "print(\"RMSE on validation: \", acc_decision_tree_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7af2b0335229cd830f0782d73d6ba06b40f382b"
   },
   "outputs": [],
   "source": [
    "decision_tree_on_log = DecisionTreeRegressor()\n",
    "decision_tree_on_log.fit(X_train, Y_train_log)\n",
    "\n",
    "acc_decision_tree_train = rmse_score(Y_train.values, np.exp(decision_tree_on_log.predict(X_train)))\n",
    "acc_decision_tree_validate = rmse_score(Y_validate.values, np.exp(decision_tree_on_log.predict(X_validate)))\n",
    "\n",
    "print(\"RMSE on train: \", acc_decision_tree_train)\n",
    "print(\"RMSE on validation: \", acc_decision_tree_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce8e64af16259a6eadadb96670e0c979b02cbb62"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e44d06153aee06ea5e5b9f79cc21b2c417757797"
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "acc_random_forest_train = rmse_score(Y_train.values, random_forest.predict(X_train))\n",
    "acc_random_forest_validate = rmse_score(Y_validate.values, random_forest.predict(X_validate))\n",
    "\n",
    "print(\"RMSE on train: \", acc_random_forest_train)\n",
    "print(\"RMSE on validation: \", acc_random_forest_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d492cc26164e9393637633acaf1547f4e0e5916c"
   },
   "outputs": [],
   "source": [
    "random_forest_on_log = RandomForestRegressor(n_estimators=100)\n",
    "random_forest_on_log.fit(X_train, Y_train_log)\n",
    "\n",
    "acc_random_forest_train = rmse_score(Y_train.values, np.exp(random_forest_on_log.predict(X_train)))\n",
    "acc_random_forest_validate = rmse_score(Y_validate.values, np.exp(random_forest_on_log.predict(X_validate)))\n",
    "\n",
    "print(\"RMSE on train: \", acc_random_forest_train)\n",
    "print(\"RMSE on validation: \", acc_random_forest_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c7e9f51b4fe9142e50b05cdd8905919d6203e7d"
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d150347e123425f51856cb57f87ff32623f6f2a"
   },
   "outputs": [],
   "source": [
    "xgboost = XGBRegressor()\n",
    "xgboost.fit(X_train, Y_train)\n",
    "\n",
    "acc_xgboost_train = rmse_score(Y_train.values, xgboost.predict(X_train))\n",
    "acc_xgboost_validate = rmse_score(Y_validate.values, xgboost.predict(X_validate))\n",
    "\n",
    "print(\"RMSE on train: \", acc_xgboost_train)\n",
    "print(\"RMSE on validation: \", acc_xgboost_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e7335c58e46d42a878ffc30e13e0e763293ca5c"
   },
   "outputs": [],
   "source": [
    "xgboost_on_log = XGBRegressor()\n",
    "xgboost_on_log.fit(X_train, Y_train_log)\n",
    "\n",
    "acc_xgboost_train = rmse_score(Y_train.values, np.exp(xgboost_on_log.predict(X_train)))\n",
    "acc_xgboost_validate = rmse_score(Y_validate.values, np.exp(xgboost_on_log.predict(X_validate)))\n",
    "\n",
    "print(\"RMSE on train: \", acc_xgboost_train)\n",
    "print(\"RMSE on validation: \", acc_xgboost_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a744e8586310ddf4594552a78262af36df008f8b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Linear Regression',\n",
    "              'Lasso Regression',\n",
    "              'Ridge Regression',\n",
    "              'ElasticNet Regression',\n",
    "              'Random Forest', \n",
    "              'Decision Tree',\n",
    "              'XGBoost'],\n",
    "    'Train Score': [acc_lin_train,\n",
    "                    acc_lasso_train,\n",
    "                    acc_ridge_train,\n",
    "                    acc_en_train,\n",
    "                    acc_random_forest_train, \n",
    "                    acc_decision_tree_train,\n",
    "                    acc_xgboost_train],\n",
    "    'Cross-Validation Score': [acc_lin_validate,\n",
    "                    acc_lasso_validate,\n",
    "                    acc_ridge_validate,\n",
    "                    acc_en_validate,\n",
    "                    acc_random_forest_validate, \n",
    "                    acc_decision_tree_validate,\n",
    "                    acc_xgboost_validate]})\n",
    "models.sort_values(by='Cross-Validation Score', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2fe7017834029e12144101b8426e5ded5b985b0"
   },
   "source": [
    "## Improving the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b018969e39762c5d00051e204fe72d11c245ce19"
   },
   "source": [
    "### Hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b93e98a1fef6ae2a042da46d6ab239f19a4316bc"
   },
   "source": [
    "First, I will manually find the best `alpha` for the regularised regression models because that is the most important hyperparameter. \n",
    "\n",
    "Then, I will use `sklearn`'s `RandomizedSearchCV` to search for the best values of the remaining hyperparameters. `RandomizedSearchCV` chooses a possibly best combination of hyperparameters among a grid of hyperparameter ranges. It randomly samples combinations from the grid and returns the one that gave the best accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7914073edfe9480fa8eef39c1a564494bb6bcad"
   },
   "source": [
    "#### Finding the best `alpha` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21abb4835d24fa2bde10a3aab93309d72d1760d8"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def find_best_alpha(alphas, model):\n",
    "    rmse = [rmse_score(Y_validate.values, np.exp(model(alpha=alpha).fit(X_train, Y_train_log).predict(X_validate))) \n",
    "            for alpha in alphas]\n",
    "    cv_ridge = pd.Series(rmse, index = alphas)\n",
    "    cv_ridge.plot(title = \"On Validation set\")\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "\n",
    "    min_index, min_value = min(enumerate(rmse), key=operator.itemgetter(1))\n",
    "\n",
    "    print(\"Minimum RMSE =\", min_value, \"found at alpha =\", alphas[min_index])\n",
    "\n",
    "    return (alphas[min_index], min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3e8e75499ab2d611118aec4c79326ab6cb4d1c3"
   },
   "source": [
    "*Ridge regression* -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c114e4cf103a868f09a377540877f6e8423dd4c"
   },
   "outputs": [],
   "source": [
    "(ridge_alpha, _) = find_best_alpha(alphas=[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300],\n",
    "                                   model=Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "715dd1a65e22803d70e0c9d41aec7dd4c86e1090"
   },
   "source": [
    "*ElasticNet regression* -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "534e3a30c5189b928140d1fe664e002df0197449"
   },
   "outputs": [],
   "source": [
    "(elasticNet_alpha, _) = find_best_alpha(alphas=[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10, 30, 100],\n",
    "                                        model=ElasticNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba4364ec10119aed42465b564f40fa993e4fa252"
   },
   "source": [
    "*Lasso Regression* -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "642e2db055dc313e7eb5a139e1649f2cf45c32ef"
   },
   "outputs": [],
   "source": [
    "(lasso_alpha, _) = find_best_alpha(alphas=[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10, 30, 100, 300, 1000],\n",
    "                                   model=Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bca1bb02d68d9995f8aa5c2a41c714241031fc9e"
   },
   "source": [
    "#### Using `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94d9347f1d84ec889df1072a4ed6a12e02b3f28f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58f04573be6e04c12430837574f2c695a587b459"
   },
   "source": [
    "*Ridge Regression* -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee590ebc07d91e1fecb8671ae62096c93a4bf5c1"
   },
   "outputs": [],
   "source": [
    "ridge_random_grid = {'alpha': [int(x) for x in np.linspace(start=0.5*ridge_alpha, stop=1.5*ridge_alpha, num=10)],\n",
    "                     'fit_intercept': [True],\n",
    "                     'normalize': [True, False],\n",
    "                     'copy_X': [True],\n",
    "                     'max_iter': [10, 30, 100, 300, 1000, 3000, 10000],\n",
    "                     'tol': [0.0001],\n",
    "                     'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                     'random_state': [42]}\n",
    "ridge = Ridge()\n",
    "ridge_random = RandomizedSearchCV(estimator = ridge, \n",
    "                                  param_distributions = ridge_random_grid, \n",
    "                                  n_iter = 100, \n",
    "                                  verbose=1, \n",
    "                                  random_state=42, \n",
    "                                  n_jobs = -1)\n",
    "\n",
    "ridge_random.fit(X_train, Y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d24906e8c56764059320ce580cfc9138e40bc69b"
   },
   "outputs": [],
   "source": [
    "print(rmse_score(Y_validate.values, ridge_random.best_estimator_.predict(X_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0983cf5892a82014026f712276557161976bde2"
   },
   "outputs": [],
   "source": [
    "ridge_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08ce5a996fa82be44a9ae4748e0ab2fbf7f849c9"
   },
   "source": [
    "*ElasticNet Regression* -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77855a99e53ab8ee16aad08ce94524fd828a41b0"
   },
   "outputs": [],
   "source": [
    "elasticNet_random_grid = {'alpha': [int(x) for x in np.linspace(start=0.5*elasticNet_alpha, \n",
    "                                                                stop=1.5*elasticNet_alpha, \n",
    "                                                                num=10)],\n",
    "                          'l1_ratio': [0.01, 0.03, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "                          'fit_intercept': [True],\n",
    "                          'normalize': [True, False],\n",
    "                          'precompute': [True, False],\n",
    "                          'copy_X': [True],\n",
    "                          'max_iter': [10, 30, 100, 300, 1000, 3000, 10000],\n",
    "                          'tol': [0.0001],\n",
    "                          'warm_start': [True, False],\n",
    "                          'positive': [True, False],\n",
    "                          'selection': ['cyclic', 'random'],\n",
    "                          'random_state': [42]}\n",
    "\n",
    "elastic_net = ElasticNet()\n",
    "elastic_net_random = RandomizedSearchCV(estimator = elastic_net, \n",
    "                                  param_distributions = elasticNet_random_grid, \n",
    "                                  n_iter = 100, \n",
    "                                  verbose=1, \n",
    "                                  random_state=42, \n",
    "                                  n_jobs = -1)\n",
    "\n",
    "elastic_net_random.fit(X_train, Y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3976cfd5dd9993f55e731e6dcc37303051a9a674"
   },
   "outputs": [],
   "source": [
    "print(rmse_score(Y_validate.values, elastic_net_random.best_estimator_.predict(X_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e22b0eeb131f28231fdcd140487eccb486bd6bce"
   },
   "outputs": [],
   "source": [
    "elastic_net_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3fa9e7fa7309435f5a9f06ae40b0a6c6b046cf41"
   },
   "source": [
    "*Lasso Regression* -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6ad5fcbb49ae98e098c12c64ee9c11b41623205"
   },
   "outputs": [],
   "source": [
    "lasso_random_grid = {'alpha': [int(x) for x in np.linspace(start=0.5*lasso_alpha, \n",
    "                                                                stop=1.5*lasso_alpha, \n",
    "                                                                num=10)],\n",
    "                     'fit_intercept': [True],\n",
    "                     'normalize': [True, False],\n",
    "                     'precompute': [True, False],\n",
    "                     'copy_X': [True],\n",
    "                     'max_iter': [10, 30, 100, 300, 1000, 3000, 10000],\n",
    "                     'tol': [0.0001],\n",
    "                     'warm_start': [True, False],\n",
    "                     'positive': [True, False],\n",
    "                     'selection': ['cyclic', 'random'],\n",
    "                     'random_state': [42]}\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso_random = RandomizedSearchCV(estimator = lasso, \n",
    "                                  param_distributions = lasso_random_grid, \n",
    "                                  n_iter = 100, \n",
    "                                  verbose=1, \n",
    "                                  random_state=42, \n",
    "                                  n_jobs = -1)\n",
    "\n",
    "lasso_random.fit(X_train, Y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9784670582430acfb57ee025a4ce3ced85449418"
   },
   "outputs": [],
   "source": [
    "print(rmse_score(Y_validate.values, lasso_random.best_estimator_.predict(X_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6674cfb9db32903f9401c5af9f60c044e52d3565"
   },
   "outputs": [],
   "source": [
    "lasso_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a204fe6285abefe80eb4a1471d68a453678c7e3e"
   },
   "source": [
    "#### Improving `Random Forest` -\n",
    "\n",
    "The `Random Forest` model achieves the best accuracy (of __86.24%__ when I ran it locally) on the cross-validation set. \n",
    "\n",
    "**Note: _The huge difference between the `Cross-Validation Score`s of `Random Forest` and `XGBoost` as compared to`Decision_Tree` and the fact that the `Decision Tree` model reached ~100% accuracy on Training data, highlights that it is easy to overfit a `Decision Tree`. The former 2 being an ensemble of `Decision Tree`s, are inherently good at avoiding overfitting._**\n",
    "\n",
    "Now, let's try to tweak the model to further improve it.\n",
    "\n",
    "**Reducing the number of features:**\n",
    "\n",
    "A general rule of thumb in Machine Learning is that the more features you have, the greater are the chances of your model overfitting the data. Extra features can decrease performance because they may “confuse” the model by giving it irrelevant data that prevents it from learning the actual relationships.\n",
    "\n",
    "Scikit-learn's `RandomForestRegressor` has a useful attribute - `feature_importances_` - which gives each feature a score based on how important it is in the prediction process.\n",
    "\n",
    "We use it to extract the `n`-best features upon which we train our final model.\n",
    "\n",
    "We find the value of `n` by iteratively training the model on number of features in the multiple of 10s. Then, we take at the value which returned the best score on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ed2a1d431d6dce06127ae4d0284d671789f65842"
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({\n",
    "    'Features': X_train.columns,\n",
    "    'Importances': random_forest.feature_importances_\n",
    "})\n",
    "feature_importances = feature_importances.sort_values(by='Importances', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b2f78fc089464c21d335ce21d9a07b9c8b64ac2"
   },
   "outputs": [],
   "source": [
    "best = {}\n",
    "\n",
    "for feature_count in range(10, 210, 10):\n",
    "    best[feature_count] = {'feature_list': list(feature_importances['Features'][:feature_count])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae2c6403116b5af0d3b268aa7798d69c01260893"
   },
   "outputs": [],
   "source": [
    "for feature_set_size, features in best.items():\n",
    "    train_accuracy_sum = 0\n",
    "    validation_accuracy_sum = 0\n",
    "    \n",
    "    nattempts = 3\n",
    "    for attempt in range(nattempts):\n",
    "        \n",
    "        X_train_best = X_train[features['feature_list']]\n",
    "        X_validate_best = X_validate[features['feature_list']]\n",
    "        X_test_best = X_test[features['feature_list']]\n",
    "\n",
    "        random_forest = RandomForestRegressor(n_estimators=100)\n",
    "        random_forest.fit(X_train_best, Y_train)\n",
    "\n",
    "        acc_random_forest_train = rmse_score(Y_train.values, random_forest.predict(X_train_best))\n",
    "        acc_random_forest_validate = rmse_score(Y_validate.values, random_forest.predict(X_validate_best))\n",
    "        \n",
    "        train_accuracy_sum += acc_random_forest_train\n",
    "        validation_accuracy_sum += acc_random_forest_validate\n",
    "\n",
    "    features['train_accuracy_avg'] = train_accuracy_sum/nattempts\n",
    "    features['validation_accuracy_avg'] = validation_accuracy_sum/nattempts\n",
    "    \n",
    "    print(\"No. of features:\", feature_set_size)\n",
    "    print(\"Train accuracy: \", features['train_accuracy_avg'])\n",
    "    print(\"Cross-Validation accuracy: \", features['validation_accuracy_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe2d084a3bcf17ff3229387c943faacb65dba6a2"
   },
   "outputs": [],
   "source": [
    "feature_sizes_sorted = sorted(best.keys(), \n",
    "                              key=lambda feature_count: best[feature_count]['validation_accuracy_avg'])\n",
    "accuracies = [best[nfeatures]['validation_accuracy_avg'] for nfeatures in feature_sizes_sorted]\n",
    "\n",
    "for nfeatures, accuracies in zip(feature_sizes_sorted, accuracies):\n",
    "    print(nfeatures, \": \", round(accuracies, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0a21c3ec14522f8699b865eea781297479ae9989"
   },
   "source": [
    "Okay, the best validation set RMSE of `0.13736`. This is almost the same as the accuracy I got using just the default model.\n",
    "\n",
    "Well, it was a promising avenue to look into. So, its okay that it did not improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f38f882f0596998d0ccc22b4903e478bad37b1c8"
   },
   "source": [
    "## Final submission model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d57c03b2c1bd57624892724126cee0ae93abb903"
   },
   "source": [
    "For the final model, I am going to use those models that gave me an `RMS error < 0.13`i.e, __Ridge Regression__, __ElasticNet Regression__, __Lasso regression__ and the __XGBoost__ models.\n",
    "\n",
    "I have found, through trial and error, that stacking **_all those models except the Lasso model_**, gives the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1541f52d1249d2e1adc72c9c5e7aa5aef24803fb"
   },
   "outputs": [],
   "source": [
    "Y_validate_lasso_in = (ridge_random.best_estimator_.predict(X_validate) +\n",
    "                         elastic_net_random.best_estimator_.predict(X_validate) +\n",
    "                         lasso_random.best_estimator_.predict(X_validate) +\n",
    "                         xgboost.predict(X_validate)) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24195f6af2ad2efdf81ae3eb70b74a11ac96130c"
   },
   "outputs": [],
   "source": [
    "Y_validate_lasso_out = (ridge_random.best_estimator_.predict(X_validate) +\n",
    "                         elastic_net_random.best_estimator_.predict(X_validate) +\n",
    "                         #lasso_random.best_estimator_.predict(X_validate) +\n",
    "                         xgboost.predict(X_validate)) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3344e66720b38354594d651500975f098e56470d"
   },
   "outputs": [],
   "source": [
    "rmse_score(Y_validate.values, Y_validate_lasso_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f859a68c4a22c13d41a4c07183a28e27f0c92006"
   },
   "outputs": [],
   "source": [
    "rmse_score(Y_validate.values, Y_validate_lasso_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ba92ee585829c4a59901cfa3e6f2a7bbced564e"
   },
   "source": [
    "Now train the final model on the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e22ee310d1c6e74754e4fc8c5e0c16bc229cada9"
   },
   "outputs": [],
   "source": [
    "final_train = all_data[:ntrain+nvalidate][:]\n",
    "final_test = all_data[ntrain+nvalidate:][:]\n",
    "\n",
    "final_train_X = final_train.drop('SalePrice', axis= 1)\n",
    "final_train_Y = final_train['SalePrice']\n",
    "final_train_Y_log = SalePriceLog\n",
    "final_test_X = final_test.drop('SalePrice', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e92af00bab42bd460d073b46d92b68ad1b0be81b"
   },
   "outputs": [],
   "source": [
    "# make sure there are no missing features\n",
    "missing_features(final_train_X, final_train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5102c8fc53ab27d2417c3ac875a0bf691218ff1"
   },
   "outputs": [],
   "source": [
    "ridge_random_grid = {'alpha': [int(x) for x in np.linspace(start=0.5*ridge_alpha, stop=1.5*ridge_alpha, num=10)],\n",
    "                     'fit_intercept': [True],\n",
    "                     'normalize': [True, False],\n",
    "                     'copy_X': [True],\n",
    "                     'max_iter': [10, 30, 100, 300, 1000, 3000, 10000],\n",
    "                     'tol': [0.0001],\n",
    "                     'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                     'random_state': [42]}\n",
    "ridge = Ridge()\n",
    "ridge_random = RandomizedSearchCV(estimator = ridge, \n",
    "                                  param_distributions = ridge_random_grid, \n",
    "                                  n_iter = 100, \n",
    "                                  verbose=1, \n",
    "                                  random_state=42, \n",
    "                                  n_jobs = -1)\n",
    "\n",
    "ridge_random.fit(final_train_X, final_train_Y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e564762c9fd8cd694017033620b516a64fce0d0f"
   },
   "outputs": [],
   "source": [
    "elasticNet_random_grid = {'alpha': [int(x) for x in np.linspace(start=0.5*elasticNet_alpha, \n",
    "                                                                stop=1.5*elasticNet_alpha, \n",
    "                                                                num=10)],\n",
    "                          'l1_ratio': [0.01, 0.03, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "                          'fit_intercept': [True],\n",
    "                          'normalize': [True, False],\n",
    "                          'precompute': [True, False],\n",
    "                          'copy_X': [True],\n",
    "                          'max_iter': [10, 30, 100, 300, 1000, 3000, 10000],\n",
    "                          'tol': [0.0001],\n",
    "                          'warm_start': [True, False],\n",
    "                          'positive': [True, False],\n",
    "                          'selection': ['cyclic', 'random'],\n",
    "                          'random_state': [42]}\n",
    "\n",
    "elastic_net = ElasticNet()\n",
    "elastic_net_random = RandomizedSearchCV(estimator = elastic_net, \n",
    "                                  param_distributions = elasticNet_random_grid, \n",
    "                                  n_iter = 100, \n",
    "                                  verbose=1, \n",
    "                                  random_state=42, \n",
    "                                  n_jobs = -1)\n",
    "\n",
    "elastic_net_random.fit(final_train_X, final_train_Y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ef7864da74073d14063bc2bc852ebbc8e1097a8"
   },
   "outputs": [],
   "source": [
    "xgboost = XGBRegressor()\n",
    "xgboost.fit(final_train_X, final_train_Y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26c225cb7987b0b3ec4b8bf742b4857951f96a8f"
   },
   "outputs": [],
   "source": [
    "rmse_score(final_train_Y.values, ridge_random.best_estimator_.predict(final_train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e93de22c0e77bff39749daa079e1f9fa9d998b56"
   },
   "outputs": [],
   "source": [
    "rmse_score(final_train_Y.values, elastic_net_random.best_estimator_.predict(final_train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3457cb9118e862b65fff3933dc23ec808c0f0009"
   },
   "outputs": [],
   "source": [
    "rmse_score(final_train_Y.values, xgboost.predict(final_train_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fdf7c8db80e17622373d124abb5392759dcfbc51"
   },
   "source": [
    "The final predicted values - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "652b882470cfeb7eaf5c7b3a172e9b7b0b0c31b5"
   },
   "outputs": [],
   "source": [
    "final_Y_pred = np.exp((ridge_random.best_estimator_.predict(final_test_X) +\n",
    "                   elastic_net_random.best_estimator_.predict(final_test_X) +\n",
    "                   xgboost.predict(final_test_X)) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "488637e40100d95d3a885e68a48155d0b7a868d2"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'Id': test_id,\n",
    "                           'SalePrice': final_Y_pred})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
